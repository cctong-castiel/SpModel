# SENTENCEPIECE 📝
It is a tokenizer model which can handle bilingual(Cantonese & English) tokenization.

It uses sentencepiece for unsupervise training on large corpus of cantonese passages(including Engllish words). Sentencepiece is based on bert model.

For tokenization, it is done with multiprocessing to speed up the tokenization.